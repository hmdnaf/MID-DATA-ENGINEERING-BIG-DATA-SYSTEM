{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1905e206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data simulasi 50000 baris berhasil dibuat.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "# Tambahkan import multiprocessing jika ini Soal 3\n",
    "# import multiprocessing \n",
    "\n",
    "# Generasi 50000 baris data simulasi (Menggantikan pd.read_csv)\n",
    "N = 50000\n",
    "start_date = datetime(2024, 1, 1)\n",
    "\n",
    "data = {\n",
    "    'Transaction_ID': np.arange(1, N + 1),\n",
    "    'Product_Category': np.random.choice(['Electronics', 'Clothing', 'Books', 'Beauty'], N),\n",
    "    'Purchase_Amount': np.round(np.random.uniform(10, 1000, N), 2),\n",
    "    'Country': np.random.choice(['Indonesia', 'Singapura', 'Malaysia', 'Thailand'], N),\n",
    "    'Transaction_Date': [start_date + timedelta(days=np.random.randint(0, 365)) for _ in range(N)]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df['Transaction_Date'] = pd.to_datetime(df['Transaction_Date'])\n",
    "print(f\"Data simulasi {N} baris berhasil dibuat.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84a062a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulasi: Koneksi ke MongoDB Sharded Cluster.\n",
      "Simulasi Batch: 50000 dokumen di-insert sekaligus (Bulk Write).\n",
      "Penyimpanan data historis (Batch) berhasil.\n"
     ]
    }
   ],
   "source": [
    "connect_mongodb()\n",
    "\n",
    "# Konversi seluruh data untuk penyimpanan Batch\n",
    "batch_data_to_save = df.to_dict('records')\n",
    "\n",
    "bulk_insert(batch_data_to_save)\n",
    "\n",
    "print(\"Penyimpanan data historis (Batch) berhasil.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2437f619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulasi Stream: 1 dokumen baru di-insert (Insert One).\n",
      "Penyimpanan data real-time (Stream) berhasil.\n"
     ]
    }
   ],
   "source": [
    "# Buat data baru yang masuk secara real-time\n",
    "new_stream_doc = {\n",
    "\n",
    "\"Transaction_ID\": 50001,\n",
    "\"Country\": \"Indonesia\",\n",
    "\"Purchase_Amount\": 950.00,\n",
    "\"Timestamp\": \"2025-03-09T10:00:00Z\"\n",
    "\n",
    "}\n",
    "stream_insert(new_stream_doc)\n",
    "\n",
    "print(\"Penyimpanan data real-time (Stream) berhasil.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e47a6a",
   "metadata": {},
   "source": [
    "### A. Perbedaan Data Lake dan Data Warehouse\n",
    "\n",
    "\n",
    "* **Data Lake:** Menyimpan data mentah, skema fleksibel (Schema-on-Read). Digunakan untuk Data Scientist, Machine Learning.\n",
    "\n",
    "\n",
    "* **Data Warehouse:** Menyimpan data terstruktur, skema ketat (Schema-on-Write). Digunakan untuk Business Intelligence, Laporan.\n",
    "\n",
    "\n",
    "### B. MongoDB Sharding\n",
    "\n",
    "\n",
    "* **Konsep:** Memecah data ke banyak server fisik (shards) untuk skalabilitas horizontal.\n",
    "\n",
    "\n",
    "* **Fungsi MongoDB:** MongoDB menggunakan **Shard Key** (misalnya kolom Country) untuk menentukan di server mana data disimpan. Ini mendistribusikan beban I/O.\n",
    "\n",
    "\n",
    "* **Manfaat:** Memungkinkan pertumbuhan data tak terbatas dan peningkatan performa query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3348287b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File output_new_stream_data.json berhasil dibuat.\n"
     ]
    }
   ],
   "source": [
    "# Ubah dokumen stream baru menjadi DataFrame untuk disimpan\n",
    "new_stream_df = pd.DataFrame([new_stream_doc])\n",
    "new_stream_df.to_json('output_new_stream_data.json', orient='records', lines=True)\n",
    "print(\"File output_new_stream_data.json berhasil dibuat.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "038d0ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulasi: Koneksi ke MongoDB Sharded Cluster.\n",
      "Simulasi Batch: 100 dokumen di-insert sekaligus (Bulk Write).\n",
      "Simulasi Stream: 1 dokumen baru di-insert (Insert One).\n"
     ]
    }
   ],
   "source": [
    "# Fungsi Simulasi Operasi MongoDB\n",
    "def connect_mongodb():\n",
    "    print(\"Simulasi: Koneksi ke MongoDB Sharded Cluster.\") \n",
    "def bulk_insert(data):\n",
    "    print(f\"Simulasi Batch: {len(data)} dokumen di-insert sekaligus (Bulk Write).\")\n",
    "def stream_insert(doc):\n",
    "    print(f\"Simulasi Stream: 1 dokumen baru di-insert (Insert One).\")\n",
    "\n",
    "connect_mongodb()\n",
    "\n",
    "# Simulasi Penyimpanan Batch (Bulk Write)\n",
    "batch_data_to_save = df.head(100).to_dict('records')\n",
    "bulk_insert(batch_data_to_save)\n",
    "\n",
    "# Simulasi Penyimpanan Stream (Insert One)\n",
    "new_stream_doc = {\n",
    "    \"Transaction_ID\": 50001,\n",
    "    \"Country\": \"Indonesia\",\n",
    "    \"Purchase_Amount\": 950.00,\n",
    "    \"Timestamp\": \"2025-03-09T10:00:00Z\"\n",
    "}\n",
    "stream_insert(new_stream_doc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
